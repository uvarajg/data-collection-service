# File Naming and Storage Strategy Update

## âœ… Changes Implemented

### 1. Simplified File Naming Convention
Removed `set_X` prefixes for cleaner, more intuitive naming:

#### **Before (Old Naming):**
- `set_a_raw_combined_20250914_101812.json` (Raw data from GitHub)
- `set_b_extracted_20250914_101812.json` (Extracted fields)
- `set_c_filtered_2b_20250914_101812.json` (Filtered > $2B)
- `set_d_enriched_yfinance_20250914_101812.json` (Final enriched data)

#### **After (New Naming):**
- `raw_combined_20250914_104110.json` (Raw data from GitHub)
- *(No intermediate files saved)*
- `enriched_yfinance_20250914_104110.json` (Final enriched data)

### 2. Optimized Storage Strategy
- **âœ… JSON Only**: Removed all CSV outputs, keeping only JSON format
- **âœ… Essential Files**: Only save raw data and final enriched data
- **âœ… No Intermediate Files**: Steps 2 & 3 process data in memory without saving

### 3. Long-term Storage with Compression
Implemented intelligent file retention strategy:

#### **Files Kept Forever (with compression > 7 days old):**
- `raw_combined_*.json` â†’ `raw_combined_*.json.gz` (after 7 days)
- `enriched_yfinance_*.json` â†’ `enriched_yfinance_*.json.gz` (after 7 days)

#### **Files Auto-Deleted (after 7 days):**
- `enriched_intermediate_*.json` (batch processing saves)
- `failed_tickers_*.json` (retry lists)
- `input_source_data_job_summary_*.json` (job summaries - 7 day history)
- All legacy `set_*.json` and `*.csv` files
- Old `data_collection_summary_*.json` files (legacy naming)

## ğŸ› ï¸ New Tools

### `compress_old_files.py`
Automated compression and cleanup utility:

```bash
# Compress and cleanup old files
python compress_old_files.py

# Decompress a specific file
python compress_old_files.py --decompress file.json.gz

# Load latest enriched data (handles compression automatically)
python compress_old_files.py --load
```

**Compression Benefits:**
- **JSON files**: ~80-90% size reduction
- **Automatic handling**: Compression/decompression is transparent
- **Space efficient**: Keep years of data in compressed format

### Updated Collection Scripts

#### `collect_us_market_stocks.py`
- Uses new file naming convention
- No CSV outputs
- Only saves essential files

#### `collect_yfinance_batch.py`
- Updated to work with new file names
- Can load from compressed files automatically

## ğŸ“Š Storage Efficiency

### **Before:**
- 4 JSON files per run (raw, extracted, filtered, enriched)
- 4 CSV files per run
- **Total: 8 files Ã— 10MB = ~80MB per day**
- **Monthly: ~2.4GB**

### **After:**
- 2 JSON files per run (raw, enriched)
- Compressed after 7 days (80-90% reduction)
- **Total: 2 files Ã— 8MB = ~16MB per day**
- **Monthly: ~480MB (uncompressed week) + ~100MB (3 compressed weeks) = ~580MB**
- **ğŸ¯ 76% storage reduction!**

## ğŸš€ Integration Updates

### GitHub Actions Workflow
Updated `.github/workflows/daily_stock_collection.yml`:
- Uses new file naming convention
- Runs compression utility instead of simple deletion
- Updated success checks for new file names

### Data Loading Functions
The compression utility provides seamless data loading:

```python
from compress_old_files import load_latest_enriched_data

# Automatically loads latest file (compressed or not)
stocks = load_latest_enriched_data()
print(f"Loaded {len(stocks)} stocks")
```

## ğŸ“ˆ Benefits Summary

1. **ğŸ§¹ Cleaner file names** - No confusing set_X prefixes
2. **ğŸ’¾ 75% less storage** - Only essential files + compression
3. **âš¡ Faster processing** - No intermediate file I/O
4. **ğŸ”„ Transparent compression** - Automatic handling of compressed files
5. **ğŸ“¦ Forever retention** - Raw data and enriched data kept indefinitely
6. **ğŸ—‘ï¸ Smart cleanup** - Automatic removal of temporary files

## ğŸ”§ Migration Guide

### For Existing Code:
Replace old file patterns:
```python
# Old
files = glob.glob("set_d_enriched_yfinance_*.json")

# New
files = glob.glob("enriched_yfinance_*.json*")  # Includes .gz files
```

### For Data Loading:
Use the new compression-aware loader:
```python
# Instead of manual file loading
data = load_latest_enriched_data()
```

The system now provides a cleaner, more efficient approach to data storage while maintaining full historical data retention through compression.