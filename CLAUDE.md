# Data Collection Service - Claude Instructions

## Service Identity
**Service Name**: Data Collection Service
**Repository**: /workspaces/data-collection-service
**Language**: Python 3.11+
**Architecture**: Microservice (Part of AlgoAlchemist ecosystem)
**Current Status**: Production-ready with 99.9% success rate for US market data ‚úÖ

## Service Mission
This service is the **single source of truth** for all external data ingestion in the AlgoAlchemist trading platform. Latest implementation now provides:
- **99.9% collection success rate** for US market data (2,075/2,077 stocks)
- **Complete US market coverage** with 7,038 raw stocks filtered to 2,077 stocks >$2B
- **Automated daily collection** via GitHub Actions at 8 PM EST
- **Intelligent compression** with 76% storage savings and forever retention
- **Zero configuration required** for US market data collection

## Core Objectives ‚úÖ ACHIEVED & EVOLVED
1. **‚úÖ Consolidate** all external API calls ‚Üí Now includes GitHub + YFinance for US market
2. **‚úÖ Optimize** API usage ‚Üí 99.9% success rate with parallel processing
3. **‚úÖ Standardize** data formats ‚Üí Clean JSON with comprehensive stock data
4. **‚úÖ Self-improve** strategies ‚Üí Automated daily collection with email monitoring
5. **‚úÖ Provide** 99.9% reliability ‚Üí Exceeded all targets with US market coverage
6. **üÜï US Market Mastery** ‚Üí Complete coverage of 2,077 stocks >$2B market cap

## Service Responsibilities
‚úÖ **In Scope**:
- Collect comprehensive US market data from GitHub repositories + YFinance
- Process 7,038 raw stocks and filter to 2,077 stocks with market cap >$2B
- Handle rate limiting and retry logic with ThreadPoolExecutor parallel processing
- Intelligent file compression and storage management (76% space savings)
- Automated daily collection via GitHub Actions (8 PM EST)
- Monitor collection success rates and send email notifications
- Provide clean JSON data format with timestamped job summaries
- Legacy: Alpaca API and Google Sheets integration (maintained but not primary)

‚ùå **Out of Scope**:
- Data storage (handled by Historical Data Service)
- Data analysis or predictions (handled by AI/ML services)
- Trading decisions (handled by Trading Service)
- UI/visualization (handled by Web UI)

## Technical Stack
- **Language**: Python 3.11+
- **Framework**: FastAPI (async REST API)
- **Cache**: Redis (for data caching)
- **Queue**: RabbitMQ/Kafka (for event streaming)
- **Database**: PostgreSQL (for metadata)
- **Monitoring**: Prometheus + Grafana
- **Testing**: pytest, pytest-asyncio
- **Deployment**: Docker, Kubernetes

## Data Sources ‚≠ê UPDATED ARCHITECTURE

### Primary US Market Data Sources
1. **GitHub Repositories** (Raw Stock Discovery)
   - AMEX: https://github.com/rreichel3/US-Stock-Symbols/blob/main/amex/amex_full_tickers.json
   - NASDAQ: https://github.com/rreichel3/US-Stock-Symbols/blob/main/nasdaq/nasdaq_full_tickers.json
   - NYSE: https://github.com/rreichel3/US-Stock-Symbols/blob/main/nyse/nyse_full_tickers.json
   - **7,038 raw stocks** with market cap and sector information

2. **YFinance API** (Primary Data Enrichment)
   - Comprehensive stock data (OHLCV, fundamentals, technical indicators)
   - **99.9% success rate** (2,075/2,077 stocks)
   - Market cap validation and classification
   - No API key required, respectful rate limiting

### Legacy Data Sources (Maintained)
3. **Alpaca API** (Legacy/Optional)
   - Market data (OHLCV) for specialized use cases
   - Technical indicators
   - Real-time quotes

4. **Google Sheets** (Legacy/Optional)
   - Historical ticker list management
   - Configuration parameters

## Self-Evolution Capabilities
The service should continuously improve through:

### Learning Metrics
- API response times
- Rate limit patterns
- Data quality scores
- Cache hit rates
- Error patterns

### Evolution Triggers
- Success rate < 95%
- Latency > 200ms average
- Cache hit rate < 80%
- New API limits detected
- Data quality degradation

### Autonomous Improvements
- Adjust request timing per API
- Switch data sources based on quality
- Optimize cache TTL based on volatility
- Propose new data sources
- Self-heal from failures

## API Design
```python
# Core endpoints
GET  /health                 # Service health check
GET  /metrics                # Prometheus metrics
POST /collect/{ticker}       # Collect data for ticker
GET  /data/{ticker}          # Get cached data
POST /collect/batch          # Batch collection
GET  /sources                # List available sources
PUT  /config                 # Update configuration
GET  /quality/{source}       # Source quality metrics
```

## Data Flow
```
External APIs ‚Üí Rate Limiter ‚Üí Validator ‚Üí Normalizer ‚Üí Cache ‚Üí Event Stream
                     ‚Üì              ‚Üì           ‚Üì         ‚Üì
                  Metrics      Quality Score  Storage  Other Services
```

## Performance Requirements
- **Latency**: < 200ms p95
- **Throughput**: 1000 requests/sec
- **Availability**: 99.9% uptime
- **Cache Hit Rate**: > 80%
- **Data Freshness**: < 1 minute for real-time data

## Error Handling Strategy
1. **Retry with exponential backoff**
2. **Circuit breaker for failing sources**
3. **Fallback to alternative sources**
4. **Cache stale data with warning**
5. **Alert on persistent failures**

## Development Guidelines

### Code Structure
```python
src/
‚îú‚îÄ‚îÄ api/              # FastAPI routes
‚îú‚îÄ‚îÄ collectors/       # Data source collectors
‚îú‚îÄ‚îÄ models/          # Data models (Pydantic)
‚îú‚îÄ‚îÄ cache/           # Caching layer
‚îú‚îÄ‚îÄ validators/      # Data validation
‚îú‚îÄ‚îÄ normalizers/     # Data normalization
‚îú‚îÄ‚îÄ monitoring/      # Metrics and health
‚îú‚îÄ‚îÄ config/          # Configuration management
‚îî‚îÄ‚îÄ main.py          # Application entry point
```

### Testing Requirements
- Unit test coverage > 90%
- Integration tests for each data source
- Load testing for performance validation
- Chaos testing for resilience

### Documentation Standards
- Docstrings for all functions
- API documentation (OpenAPI/Swagger)
- Architecture decision records (ADRs)
- Runbook for operations

## Integration Points
- **Publishes to**: Event stream (Kafka/RabbitMQ)
- **Consumed by**: All downstream services
- **Depends on**: Configuration Service, Super Agent
- **Monitored by**: Monitoring Service

## Security Considerations
- API keys in environment variables
- TLS for all external connections
- Rate limiting per client
- Input validation for all endpoints
- Audit logging for all operations

## Deployment Strategy
- Containerized with Docker
- Horizontal scaling with Kubernetes
- Rolling updates with zero downtime
- Health checks and readiness probes
- Automated rollback on failures

## Success Metrics
- **Collection Success Rate**: > 99%
- **Average Latency**: < 150ms
- **Cache Hit Rate**: > 85%
- **Cost per API Call**: Decrease 20% monthly
- **Data Quality Score**: > 95%
- **Service Uptime**: > 99.9%

## Anti-Patterns to Avoid
- ‚ùå Direct database writes (use events)
- ‚ùå Synchronous long-running operations
- ‚ùå Hardcoded API endpoints
- ‚ùå Ignoring rate limits
- ‚ùå Silent failures
- ‚ùå Memory leaks from cache growth
- ‚ùå Tight coupling with other services

## üö® CRITICAL FIXES IMPLEMENTED ‚úÖ

### 1. Data Collection Pipeline Order (FIXED)
**‚ùå Previous (Wrong) Order:**
```
1. Collect OHLCV ‚Üí 2. Technical indicators ‚Üí 3. VALIDATE (move to errors) ‚Üí 4. Add fundamentals (missed!)
```
**‚úÖ Current (Fixed) Order:**
```
1. Collect OHLCV ‚Üí 2. Technical indicators ‚Üí 3. Add fundamentals ‚Üí 4. VALIDATE (with complete data)
```
**Impact**: Fixed 477 records losing fundamental data

### 2. Enhanced Fundamentals with Fallbacks (IMPLEMENTED)
**Component-Based Calculations Added:**
```python
# Debt-to-Equity Fallback Chain:
info.debtToEquity ‚Üí balance_sheet(Total Debt/Total Equity) ‚Üí quarterly_balance_sheet ‚Üí null

# Current Ratio Fallback Chain:  
info.currentRatio ‚Üí balance_sheet(Current Assets/Current Liabilities) ‚Üí quarterly_balance_sheet ‚Üí null

# Profit Margin Fallback Chain:
info.profitMargins ‚Üí financials(Net Income/Total Revenue) ‚Üí quarterly_financials ‚Üí null
```

### 3. Relaxed Technical Validation Thresholds (ADJUSTED)
**‚ùå Previous (Too Strict):**
```python
'bb_middle': (0.9, 1.1),  # Only ¬±10% - caused 25% false positives
```
**‚úÖ Current (Production-Ready):**
```python
'bb_middle': (0.7, 1.4),  # ¬±30% range - reduced to 3% false positives
```

### 4. Dividend Yield Intelligence (ENHANCED)
**Smart Null vs Zero Distinction:**
```python
# Non-dividend payers: 0.0 (companies that don't pay dividends)
# Missing data: null (API/calculation issues)
# Logic: Check dividend history to distinguish between the two
```

## üìä Current Performance Metrics ‚úÖ
- **Collection Success Rate**: 99.9% (Target: >95% ‚úÖ)
- **Error Rate**: 0.1% (Target: <5% ‚úÖ)
- **Technical Indicator Coverage**: 99.7% (Target: >95% ‚úÖ)
- **Fundamental Data Coverage**: 100% (Target: >90% ‚úÖ)
- **Data Quality Grade**: A+ across all recent dates
- **Recovery Success Rate**: 100% (technical indicator restoration)

## üèóÔ∏è Current Architecture Patterns

### US Market Data Collection (New Primary System) ‚úÖ
```python
# Main US market data collection script
collect_us_market_stocks.py

class USMarketDataCollector:
    def __init__(self):
        self.github_sources = {
            'AMEX': 'https://github.com/rreichel3/US-Stock-Symbols/blob/main/amex/amex_full_tickers.json',
            'NASDAQ': 'https://github.com/rreichel3/US-Stock-Symbols/blob/main/nasdaq/nasdaq_full_tickers.json',
            'NYSE': 'https://github.com/rreichel3/US-Stock-Symbols/blob/main/nyse/nyse_full_tickers.json'
        }

    def step1_download_raw_data(self):
        # Download 7,038 raw stocks from GitHub

    def step2_extract_fields(self):
        # Extract ticker, market_cap, country, industry, sector

    def step3_filter_by_market_cap(self):
        # Filter to 2,077 stocks with market cap > $2B

    def step4_enrich_with_yfinance(self):
        # Parallel processing with ThreadPoolExecutor
        # 99.9% success rate (2,075/2,077)
```

### Intelligent File Management ‚úÖ
```python
# compress_old_files.py - Storage optimization
def cleanup_old_files(base_path="/workspaces/data/input_source", days_old=7):
    # Keep forever (with compression):
    keep_patterns = [
        "raw_combined_*.json",      # Raw GitHub data
        "enriched_yfinance_*.json"  # Final enriched data
    ]

    # Auto-delete after 7 days:
    delete_patterns = [
        "input_source_data_job_summary_*.json",  # 7-day history
        "failed_tickers_*.json",                 # Retry logs
        "set_*.json", "*.csv"                    # Legacy files
    ]
```

### GitHub Actions Automation ‚úÖ
```yaml
# .github/workflows/daily_stock_collection.yml
name: Daily Stock Data Collection
on:
  schedule:
    - cron: '0 1 * * *'  # 8 PM EST daily

steps:
  - name: Run stock data collection
    run: python collect_us_market_stocks.py

  - name: Check collection results
    run: |
      # Verify 2,075+ stocks collected
      LATEST_FILE=$(ls -t data/input_source/enriched_yfinance_*.json | head -1)
      STOCK_COUNT=$(python -c "import json; data=json.load(open('$LATEST_FILE')); print(len(data))")

  - name: Compress old data files
    run: python compress_old_files.py
```

## AI Assistant Instructions ‚≠ê UPDATED

### üåü PRIMARY WORKFLOW: US Market Data Collection
When working on this service, **prioritize the organized script structure**:

1. **‚úÖ USE ORGANIZED SCRIPTS**: All scripts moved to `scripts/` directory with clear categorization
2. **‚úÖ MAIN PRODUCTION SCRIPTS**: Located in `scripts/main/` for core operations
3. **‚úÖ UTILITY SCRIPTS**: Located in `scripts/utils/` for maintenance and monitoring
4. **‚úÖ NEW COLLECTION SYSTEM**: `scripts/main/collect_us_market_stocks.py` is the primary method
5. **‚úÖ GITHUB + YFINANCE ARCHITECTURE**: Raw data from GitHub, enriched with YFinance
6. **‚úÖ CLEAN FILE NAMING**: `raw_combined_*.json` and `enriched_yfinance_*.json`
7. **‚úÖ ENHANCED ARCHIVING**: `scripts/main/archive_historical_data.py` with 90% compression achieved
8. **‚úÖ 99.9% SUCCESS TARGET**: Expect 2,075/2,077 stocks successfully processed
9. **‚úÖ PARALLEL PROCESSING**: ThreadPoolExecutor with respectful rate limiting
10. **‚úÖ GITHUB ACTIONS**: Daily automation at 8 PM EST with email notifications

### Legacy Instructions (Maintained for Historical Context)
8. **‚úÖ PIPELINE ORDER**: Always add fundamentals BEFORE technical validation
9. **‚úÖ USE FALLBACK CALCULATIONS**: For debt-to-equity, current ratio, profit margin
10. **‚úÖ USE RELAXED VALIDATION**: Current thresholds are production-tested
11. **‚úÖ USE CENTRALIZED UTILITIES**: retry_decorator, logging_config, settings
12. **‚úÖ PRESERVE ENHANCEMENTS**: Don't revert the critical fixes

### Production Standards
13. Always consider rate limits and costs
14. Implement comprehensive error handling with structured logging
15. Add metrics for everything measurable
16. Focus on reliability over features (99.9% success rate achieved)
17. Optimize for production from day one

## Available Prompts & Scripts

### üìä Daily Input Data Refresh
**File**: `INPUT_DATA_DAILY_REFRESH_PROMPT.md`
- Downloads raw US market data from GitHub
- Filters stocks with market cap >$2B
- Enriches with YFinance data
- Creates timestamped enriched JSON files

### üöÄ Interactive Data Collection Runner
**File**: `DATA_COLLECTION_RUN_PROMPT.md` & `scripts/main/run_data_collection_with_dates.py`
- Interactive date input (single date or range)
- Runs collection for all stocks in enriched dataset
- Uses enriched fundamentals to avoid API calls
- Provides real-time progress and summary

**Usage**:
```bash
# Interactive mode (recommended)
python scripts/main/run_data_collection_with_dates.py

# Then enter date(s) when prompted:
# Single date: 2025-09-16
# Date range: 2025-09-01 2025-09-16
```

### üìà US Market Data Collection
**File**: `scripts/main/collect_us_market_stocks.py`
- Primary script for enriched data collection
- Processes 7,038 raw stocks ‚Üí 2,077 stocks >$2B
- 99.9% success rate with parallel processing
- Automated via GitHub Actions daily at 8 PM EST

### üì¶ Historical Data Archiving
**File**: `scripts/main/archive_historical_data.py`
- Archives historical data before specified cutoff date
- Compresses data into tar.gz format (90% space savings achieved)
- Organizes archives by ticker/year/month structure
- Includes verification and integrity checking
- Dry-run mode for safe testing before archiving
- Auto-confirm flag for automated execution

**Usage**:
```bash
# Dry run to see what would be archived (recommended first)
python scripts/main/archive_historical_data.py --cutoff-date 2025-09-14 --dry-run

# Archive data on or before 2025-09-14 (inclusive)
python scripts/main/archive_historical_data.py --cutoff-date 2025-09-14 --yes

# Custom paths and cutoff date
python scripts/main/archive_historical_data.py \
  --cutoff-date 2025-08-31 \
  --base-path /workspaces/data/historical/daily \
  --archive-path /workspaces/data/archives/historical/daily \
  --yes

# Help and all options
python scripts/main/archive_historical_data.py --help
```

**Archive Structure**:
```
/workspaces/data/archives/historical/daily/
‚îú‚îÄ‚îÄ AAPL/
‚îÇ   ‚îú‚îÄ‚îÄ 2024/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AAPL_2024_01.tar.gz
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AAPL_2024_02.tar.gz
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ 2025/
‚îî‚îÄ‚îÄ MSFT/
    ‚îî‚îÄ‚îÄ 2024/
        ‚îú‚îÄ‚îÄ MSFT_2024_01.tar.gz
        ‚îî‚îÄ‚îÄ ...
```

## Data Storage Management

### Archiving Strategy
The service implements intelligent data archiving to balance storage efficiency with data accessibility:

1. **Compression**: Tar.gz format provides ~76% space savings
2. **Organization**: Archives organized by ticker/year/month for easy retrieval
3. **Verification**: Each archive is verified before original files are deleted
4. **Retention**: Original data structure preserved within archives
5. **Safety**: Dry-run mode prevents accidental data loss

### Archiving Workflow
```bash
# Recommended monthly archiving workflow
1. python scripts/main/archive_historical_data.py --cutoff-date YYYY-MM-DD --dry-run  # Test first
2. python scripts/main/archive_historical_data.py --cutoff-date YYYY-MM-DD --yes      # Execute
3. Verify archives created in /workspaces/data/archives/historical/daily/
4. Monitor storage space savings and archive integrity
```

### Archive Retrieval
```bash
# Extract specific archive
cd /workspaces/data/archives/historical/daily/AAPL/2024/
tar -xzf AAPL_2024_01.tar.gz

# List archive contents without extracting
tar -tzf AAPL_2024_01.tar.gz

# Extract single file from archive
tar -xzf AAPL_2024_01.tar.gz AAPL/2024/01/2024-01-15.json
```

## üö® Data Quality Management & Recovery Procedures ‚≠ê NEW

### Data Quality Monitoring
The service maintains A+ grade data quality through continuous monitoring and automated recovery procedures.

**Quality Metrics:**
- **Technical Indicator Coverage**: Target >95%, Current: 99.7%
- **Fundamental Data Coverage**: Target >90%, Current: 100%
- **Overall Quality Score**: Target >95%, Current: 99.9%

### Quality Validation Tool
```bash
# Validate single date
python scripts/utils/data_quality/validate_data_quality.py 2025-09-18

# Validate date range
python scripts/utils/data_quality/validate_data_quality.py --range 2025-09-15 2025-09-18

# Interactive mode
python scripts/utils/data_quality/validate_data_quality.py
```

### Technical Indicator Recovery Procedure ‚úÖ PROVEN SOLUTION

**When to Use:** If technical indicators are missing or incomplete (coverage <95%)

**üîß Recovery Command:**
```bash
# For specific date
python scripts/utils/data_quality/fix_technical_indicators_alpaca.py 2025-09-18

# Interactive mode (prompts for date)
python scripts/utils/data_quality/fix_technical_indicators_alpaca.py
```

**üéØ Recovery Process:**
1. **Scans** target date files for missing/incomplete technical indicators
2. **Retrieves** historical data from Alpaca API (70-day lookback)
3. **Calculates** 14 comprehensive technical indicators using pandas:
   - RSI (14-day), MACD (12,26,9), SMA (20,50), EMA (12,26)
   - Bollinger Bands, ATR, Volume ratios, Price positioning
4. **Merges** indicators into existing files (preserves fundamentals)
5. **Validates** final quality and provides summary

**‚ö° Performance:**
- **Processing Speed**: ~2,000 files in 15-20 minutes
- **Success Rate**: 99.7% (2,098/2,105 files enhanced)
- **API Efficiency**: Respects Alpaca rate limits (200 req/min)
- **Error Rate**: 0% (robust error handling with fallbacks)

**üìä Example Recovery Results:**
```
Before: 330 files with technical indicators (15.7% coverage)
After:  2,098 files with technical indicators (99.7% coverage)
Grade:  F ‚Üí A+ (complete restoration)
```

### Data Structure Compatibility
The recovery tool handles multiple data formats:
- **Modern Format**: `technical_indicators` section (Sept 18 style)
- **Legacy Format**: `technical` section (Sept 15-16 style)
- **Hybrid**: Automatically detects and preserves existing structure

### Common Recovery Scenarios

**Scenario 1: Complete Technical Indicator Loss**
```bash
# Symptom: 0% technical coverage after collection issues
python scripts/utils/data_quality/fix_technical_indicators_alpaca.py [date]
# Result: Restores to 99%+ coverage in 15-20 minutes
```

**Scenario 2: Partial Technical Indicator Coverage**
```bash
# Symptom: <95% technical coverage, some files missing indicators
python scripts/utils/data_quality/fix_technical_indicators_alpaca.py [date]
# Result: Fills gaps, achieves 99%+ coverage
```

**Scenario 3: Data Quality Regression**
```bash
# Symptom: Quality grade drops from A+ to B/C
# Step 1: Validate current state
python scripts/utils/data_quality/validate_data_quality.py [date]
# Step 2: Identify issues (technical vs fundamental)
# Step 3: Apply appropriate recovery tool
python scripts/utils/data_quality/fix_technical_indicators_alpaca.py [date]
```

### Recovery Success Metrics ‚úÖ PROVEN
- **September 18, 2025**: 0% ‚Üí 99.7% technical coverage (1,768 files enhanced)
- **Zero Errors**: 100% success rate across 1,775 API calls
- **Processing Time**: 17.3 minutes for 2,105 files
- **Data Integrity**: 100% preservation of existing fundamentals

## üöÄ Daily Pipeline Automation ‚≠ê PRODUCTION READY

### Pipeline Execution
The service includes a complete daily automation pipeline:

```bash
# Execute daily pipeline
./run_daily_pipeline.sh

# Pipeline steps:
# 1. Refresh US market input data (2,077 stocks)
# 2. Generate input data report
# 3. Run data collection for target date
# 4. Generate collection report
# 5. Run data validation
# 6. Generate validation report
# 7. Email all reports
```

### Script Compatibility ‚úÖ VERIFIED
All scripts have been verified for compatibility with organized structure:

**Main Production Scripts:**
- ‚úÖ `scripts/main/collect_us_market_stocks.py` - US market collection
- ‚úÖ `scripts/main/run_data_collection_with_dates.py` - Interactive + CLI collection
- ‚úÖ `scripts/main/daily_pipeline_automation.py` - Pipeline orchestration
- ‚úÖ `scripts/main/archive_historical_data.py` - Data archiving

**Data Quality Scripts:**
- ‚úÖ `scripts/utils/data_quality/validate_data_quality.py` - Quality validation
- ‚úÖ `scripts/utils/data_quality/fix_technical_indicators_alpaca.py` - Recovery tool

### Pipeline Features
- **Automatic Date Calculation**: Determines previous business day for data collection
- **Comprehensive Reporting**: JSON reports saved and emailed
- **Error Recovery**: Continues processing even if individual steps fail
- **Log Management**: Automatic log rotation (keeps last 30 files)
- **Email Notifications**: Success/failure reports with detailed metrics
- **Command Line Support**: All scripts support both interactive and automated modes

## Evolution Path
1. **Phase 1**: Basic data collection with caching ‚úÖ
2. **Phase 2**: Multi-source with fallbacks ‚úÖ
3. **Phase 3**: Self-optimization features ‚úÖ
4. **Phase 4**: Data quality monitoring and recovery ‚úÖ CURRENT
5. **Phase 5**: Predictive pre-fetching (NEXT)
6. **Phase 6**: Fully autonomous operation (FUTURE)

Remember: This service is the foundation of data reliability for the entire AlgoAlchemist platform. Every decision should prioritize **reliability**, **efficiency**, and **intelligence**.